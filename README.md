# DS_Knowledge
Deep study on ML algorithms and overall ML knowledge

## Contents
Entropy  
Hierarchical Clustering  
Decision Tree  


### Entropy

<pre>What is entropy</pre>
Simply put, entropy is measure of disorder.  
![alt text](https://user-images.githubusercontent.com/48074724/134520914-b9891103-a22d-41c7-ba6a-ad266e35b9e9.png)

In the formula, Pi is the frequentist probability of an element/class 'i' in our data. Let's say we have two classes, + and -. Then i will be either + or -.  
Now, if we have 30 + and 70 -, P+ will be 3/10 and P- will be 7/10. Plug that in the formula above, then you'll get this  
![alt text]()

![alt text](https://user-images.githubusercontent.com/48074724/134523895-230e311e-1b39-44ec-beee-170239b68bd6.png)
Reference : https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8
